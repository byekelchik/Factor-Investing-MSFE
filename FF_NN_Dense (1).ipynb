{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,timedelta\n",
    "from scipy import stats\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "import seaborn as sn\n",
    "import pandas_datareader.data as reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data\n",
    "ticker = str(input('Please input ticker'))\n",
    "equity_data = yf.download(ticker, start =  '2005-01-01', end = '2020-09-01', interval = '1d', auto_adjust = 'True')\n",
    "print(equity_data.columns)\n",
    "equity_data['Pct_Returns']= equity_data[\"Close\"].pct_change(1)\n",
    "equity_data['Pct_Returns']`\n",
    "equity_data['Raw_Returns'] = equity_data['Pct_Returns']*100\n",
    "equity_data['Log_Returns'] = np.log(equity_data.Close) - np.log(equity_data.Close.shift(1))\n",
    "equity_data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = reader.DataReader('F-F_Research_Data_Factors','famafrench',start =  '2005-01-01', end = '2020-08-31')\n",
    "factors = factors[0]\n",
    "equity_returns_mt = equity_data['Pct_Returns'].resample('M').agg(lambda x:(x+1).prod()-1)\n",
    "#equity_returns_mt = equity_returns_mt[:-1]\n",
    "equity_returns_mt.index = factors.index\n",
    "print(factors.head())\n",
    "print(equity_returns_mt.head())\n",
    "print(factors.shape)\n",
    "print(equity_returns_mt.shape)\n",
    "full_dataset = pd.merge(factors,equity_returns_mt,on = 'Date')\n",
    "full_dataset[['Mkt-RF','SMB','HML','RF']]= full_dataset[['Mkt-RF','SMB','HML','RF']]/100 #coverting from percent to raw value \n",
    "full_dataset['Excess_Returns']=full_dataset['Pct_Returns'] - full_dataset['RF'] #Excess Return = Portfolio - RF\n",
    "full_dataset=full_dataset.rename(columns = {'Pct_Returns':ticker + ' Pct_Returns'})\n",
    "full_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing price\n",
    "plt.title(str(ticker) +' Log Returns')\n",
    "plt.plot(equity_data['Log_Returns'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.shape\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount of data to train,\n",
    "forecast = 12\n",
    "training_close_length = (len(full_dataset)- 12)/len(full_dataset)\n",
    "#training_close_length\n",
    "print((math.ceil((1/20)*len(full_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = full_dataset[\"Excess_Returns\"]\n",
    "x_values = full_dataset.iloc[:,0:3]\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_values, y_values, test_size = (math.ceil((1/20)*len(full_dataset))),\n",
    "                                                 shuffle = False,\n",
    "                                                 )\n",
    "x_train.iloc[:len(x_train),:]\n",
    "(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "sc = MinMaxScaler((-1,1))\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_train, x_test = np.array(x_train), np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_test = np.array(y_train).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "y_train = sc.fit_transform(y_train)\n",
    "y_test = sc.fit_transform(y_test)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEl \n",
    "def define_model(lr=.01,\n",
    "                 nodes_2=64,\n",
    "                 nodes_3=36,\n",
    "                 drop_rate_1 = .3,\n",
    "                 drop_rate_2 = .3,\n",
    "                 activation_2='sigmoid', activation_3='sigmoid'):\n",
    "    model = Sequential ()\n",
    "    #model.add(Dense(nodes_1,input_dim = 3,\n",
    "    #              activation = activation_1,)),\n",
    "    #model.add(Dropout(.3)),\n",
    "    model.add(Dense(nodes_2,\n",
    "                    input_dim = 3,\n",
    "                   activation = activation_2)),\n",
    "    model.add(Dropout(rate=drop_rate_1)),\n",
    "    model.add(Dense(nodes_3,\n",
    "                    #input_dim = 3,\n",
    "                   activation = activation_3)),\n",
    "    model.add(Dropout(rate=drop_rate_2)),\n",
    "    model.add(Dense(1,\n",
    "             activation = \"linear\")) #Dense --> Default activation is linear\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss = 'mean_squared_error',\n",
    "                  metrics =['mse']) #metrics =[tf.keras.metrics.AUC()]\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(lr = .0001,\n",
    "                     nodes_2 = 64,\n",
    "                     nodes_3 = 36\n",
    "                     activation_2='tanh',\n",
    "                     activation_3='sigmoid',\n",
    "                     drop_rate_1 = .3,\n",
    "                     drop_rate_2 = .3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuning = KerasRegressor(build_fn = define_model,\n",
    "                              #epochs=80,batch_size=64,\n",
    "                              lr = .0001,\n",
    "                              #nodes_1 = 64, \n",
    "                              #nodes_2=24, \n",
    "                              #nodes_3=8,\n",
    "                              #activation_1='tanh',\n",
    "                              #activation_2='sigmoid', \n",
    "                              #activation_3='sigmoid',\n",
    "                              drop_rate_1 =.2,\n",
    "                              drop_rate_2 = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter Optimization\n",
    "lr = [.001,.005,.0001]\n",
    "#units = [[25,30],[15,10]]\n",
    "#optimizer = ['SGD','ADAM']\n",
    "#batch_size = [64]\n",
    "#epochs=[80]\n",
    "#n_nodes_1 = [64]\n",
    "#n_nodes_2 = [32,24]\n",
    "#n_nodes_3 = [16,8]\n",
    "#activation_1 = ['sigmoid','tanh'] \n",
    "#activation_2 = ['sigmoid','tanh']\n",
    "#activation_3 = ['sigmoid','tanh']\n",
    "drop_rate_1 = [.2,.3,.4]\n",
    "drop_rate_2 = [.2,.3,.4]\n",
    "param_grid = dict(#optimizer = optimizer,\n",
    "                  #batch_size= batch_size,\n",
    "                  #epochs=epochs,\n",
    "                  drop_rate_1 = drop_rate_1,\n",
    "                  drop_rate_2 = drop_rate_2,\n",
    "                  lr = lr\n",
    "                  #nodes_1 = n_nodes_1,\n",
    "                  #nodes_2= n_nodes_2,\n",
    "                  #nodes_3 = n_nodes_3,\n",
    "                  #activation_1 = activation_1,\n",
    "                  #activation_2 = activation_2,\n",
    "                  #activation_3 = activation_3\n",
    "                 )\n",
    "grid = GridSearchCV(estimator = model_tuning,param_grid = param_grid)\n",
    "grid_result = grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_result.cv_results_)\n",
    "results.to_csv('FF_Dense_HyperTuning')\n",
    "print(\"Best: %s\" % (grid_result.best_params_))\n",
    "results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 130 #HOW MANY TIMES IS THE DATA FED THROUGH THE SYSTEM\n",
    "BATCH_SIZE = 64  #HOW MUCH OF THE TRAINING DATA IS PUT THROUGH BEFORE WEIGHT UPDATES\n",
    "VALIDATION_SPLIT = .2 #HOW MUCH OF THE TRAINING SET IS SET ASIDE TO VALIDATE\n",
    "call_backs = [\n",
    "    tf.keras.callbacks.CSVLogger('FF_Dense.log',\n",
    "                                 separator = ',',\n",
    "                                 append = False), #Export performance to CSV\n",
    "    tf.keras.callbacks.EarlyStopping(monitor = 'mse',\n",
    "                                     patience = 30, \n",
    "                                     restore_best_weights = True )]\n",
    "\n",
    "model.fit(x_train, y_train,batch_size= BATCH_SIZE,\n",
    "          epochs =EPOCHS,\n",
    "          callbacks = call_backs,\n",
    "          validation_split= VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = pd.read_csv('FF_Dense.log',index_col = 'epoch')\n",
    "train_log.val_loss.plot(legend=True)\n",
    "train_log.loss.plot(legend=True)#val_loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('AAPL_Dense_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION\n",
    "import_model = load_model('Citi_Dense_MODEL') #load model\n",
    "train_prediction_unscaled = import_model.predict(x_train)\n",
    "test_prediction_unscaled = import_model.predict(x_test)\n",
    "train_prediction_unscaledtrain_prediction = sc.inverse_transform(train_prediction_unscaled) #PREDICTIONS USING TRAIN DATA\n",
    "test_prediction = sc.inverse_transform(test_prediction_unscaled) #PREDICTIONS USING TEST DATA\n",
    "test_prediction\n",
    "Citi_Predictions_Train = pd.DataFrame(train_prediction)\n",
    "Citi_Predictions_Test = pd.DataFrame(test_prediction)\n",
    "Citi_Predictions_Test.columns,Citi_Predictions_Train.columns = ['Citi_Predictions'], ['Citi_Predictions']\n",
    "#Citi_Predictions_Test.to_csv(\"Citi_Predictions_Test\")\n",
    "#Citi_Predictions_Train.to_csv(\"Citi_Prediction_Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "#rmse_test = np.sqrt(np.mean(test_prediction - y_test)**2)\n",
    "rmse_train = np.sqrt(np.mean(train_prediction - y_train)**2)\n",
    "print(('RMSE for the Test set is ') + str(rmse_test))\n",
    "print(('RMSE for the Train set is ') + str(rmse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Googl_to_CSV\n",
    "import_model = load_model('AAPL_Dense_MODEL') #load model\n",
    "train_prediction_unscaled = import_model.predict(x_train)\n",
    "test_prediction_unscaled = import_model.predict(x_test)\n",
    "train_prediction = sc.inverse_transform(train_prediction_unscaled) #PREDICTIONS USING TRAIN DATA\n",
    "test_prediction = sc.inverse_transform(test_prediction_unscaled) #PREDICTIONS USING TEST DATA\n",
    "GOOGL_Predictions_Train = pd.DataFrame(train_prediction)\n",
    "GOOGL_Predictions_Test = pd.DataFrame(test_prediction)\n",
    "GOOGL_Predictions_Test.columns,GOOGL_Predictions_Train.columns = ['GOOGL_Predictions'], ['GOOGL_Predictions']\n",
    "GOOGL_Predictions_Test.to_csv(\"GOOGL_Predictions_Test\")\n",
    "GOOGL_Predictions_Train.to_csv(\"GOOGL_Prediction_Train\")\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AAPL_to_CSV\n",
    "import_model = load_model('AAPL_Dense_MODEL') #load model\n",
    "train_prediction_unscaled = import_model.predict(x_train)\n",
    "test_prediction_unscaled = import_model.predict(x_test)\n",
    "train_prediction = sc.inverse_transform(train_prediction_unscaled) #PREDICTIONS USING TRAIN DATA\n",
    "test_prediction = sc.inverse_transform(test_prediction_unscaled) #PREDICTIONS USING TEST DATA\n",
    "AAPL_Predictions_Train = pd.DataFrame(train_prediction)\n",
    "AAPL_Predictions_Test = pd.DataFrame(test_prediction)\n",
    "AAPL_Predictions_Test.columns,AAPL_Predictions_Train.columns = ['AAPL_Predictions'], ['AAPL_Predictions']\n",
    "AAPL_Predictions_Test.to_csv(\"AAPL_Predictions_Test\")\n",
    "AAPL_Predictions_Train.to_csv(\"AAPL_Prediction_Train\")\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COKE_to_CSV\n",
    "import_model = load_model('Citi_Dense_MODEL') #load model\n",
    "train_prediction_unscaled = import_model.predict(x_train)\n",
    "test_prediction_unscaled = import_model.predict(x_test)\n",
    "train_prediction = sc.inverse_transform(train_prediction_unscaled) #PREDICTIONS USING TRAIN DATA\n",
    "test_prediction = sc.inverse_transform(test_prediction_unscaled) #PREDICTIONS USING TEST DATA\n",
    "Coke_Predictions_Train = pd.DataFrame(train_prediction)\n",
    "Coke_Predictions_Test = pd.DataFrame(test_prediction)\n",
    "Coke_Predictions_Test.columns,Coke_Predictions_Train.columns = ['Coke_Predictions'], ['Coke_Predictions']\n",
    "Coke_Predictions_Test.to_csv(\"Coke_Predictions_Test\")\n",
    "Coke_Predictions_Train.to_csv(\"Coke_Prediction_Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tesla_to_CSV\n",
    "import_model = load_model('Citi_Dense_MODEL') #load model\n",
    "train_prediction_unscaled = import_model.predict(x_train)\n",
    "test_prediction_unscaled = import_model.predict(x_test)\n",
    "test_prediction_unscaled\n",
    "Tesla_predictions = pd.DataFrame(test_prediction,prediction_time_interval)\n",
    "Tesla_predictions.columns = ['Citi_Estimates']\n",
    "Tesla_predictions.to_csv(\"Tesla_Prediction_01-01-2020_onwards\")\n",
    "Tesla_predictions.plot()\n",
    "Tesla_predictions.describe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
